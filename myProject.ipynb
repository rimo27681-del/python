{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b08ef0a-8a80-493e-a31c-f6ee0f950170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6234080-61ef-4e28-94d9-6cd12ffc665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _load_and_prepare_data(self, file_path):\n",
    "        clean_lines = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip().lower().startswith(\"@input\") and not line.strip().lower().startswith(\"@output\"):\n",
    "                    clean_lines.append(line)\n",
    "        temp_arff = file_path.replace('.dat', '.arff')\n",
    "        with open(temp_arff, \"w\") as f:\n",
    "            f.writelines(clean_lines)\n",
    "        data, meta = arff.loadarff(temp_arff)\n",
    "        df = pd.DataFrame(data)\n",
    "        # Dynamically find class column (usually last attribute in KEEL datasets)\n",
    "        class_col = meta.names()[-1] if meta is not None and len(meta.names()) > 0 else None\n",
    "        if class_col is None or class_col not in df.columns:\n",
    "            print(f\"Error: No class column found in {file_path}. Available columns: {list(df.columns)}\")\n",
    "            raise KeyError(f\"No class column found in {file_path}. Available columns: {list(df.columns)}\")\n",
    "        # Decode bytes if necessary\n",
    "        df[class_col] = df[class_col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "        le = LabelEncoder()\n",
    "        df['Class_encoded'] = le.fit_transform(df[class_col])\n",
    "        X = df.drop(columns=[class_col, 'Class_encoded'])\n",
    "        y = df['Class_encoded'].values.ravel()\n",
    "        os.remove(temp_arff)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2697a-b116-4c4d-8c41-cf88f46cbe4f",
   "metadata": {},
   "source": [
    "# Modified OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7617c48-f1d4-4729-93e5-3e8437319102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedOPTICS:\n",
    "    def __init__(self, min_samples=5, max_eps=np.inf, xi=0.05, \n",
    "                 min_cluster_size=None, predecessor_correction=True):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_eps = max_eps\n",
    "        self.xi = xi\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.predecessor_correction = predecessor_correction\n",
    "\n",
    "        # outputs\n",
    "        self.reachability_ = None\n",
    "        self.core_distances_ = None\n",
    "        self.predecessor_ = None\n",
    "        self.ordering_ = None\n",
    "        self.labels_ = None\n",
    "        self.ri_ = None  # reliable coefficient\n",
    "\n",
    "    # === Eq. 9: Reliable Coefficient ===\n",
    "    def _compute_reliable_coeff(self, X, y):\n",
    "        ri = np.full(len(X), np.inf)\n",
    "        for i in range(len(X)):\n",
    "            mask = y != y[i]   # find nearest neighbor from different class\n",
    "            if np.any(mask):\n",
    "                dists = np.linalg.norm(X[i] - X[mask], axis=1)\n",
    "                ri[i] = np.min(dists)\n",
    "        return ri\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "        self.labels_ = np.full(n, -1)\n",
    "        self.reachability_ = np.full(n, np.inf)\n",
    "        self.predecessor_ = np.full(n, -1, dtype=int)\n",
    "        self.ordering_ = []\n",
    "\n",
    "        # === Step 1: Compute reliable coefficients ===\n",
    "        self.ri_ = self._compute_reliable_coeff(X, y)\n",
    "\n",
    "        # === Step 2: Compute core distances ===\n",
    "        nn = NearestNeighbors(n_neighbors=self.min_samples)\n",
    "        nn.fit(X)\n",
    "        dists, _ = nn.kneighbors(X)\n",
    "        self.core_distances_ = dists[:, -1].copy()\n",
    "        self.core_distances_[self.core_distances_ > self.max_eps] = np.inf\n",
    "\n",
    "        processed = np.zeros(n, dtype=bool)\n",
    "        seeds = []\n",
    "        counter = 0\n",
    "\n",
    "        for point_idx in range(n):\n",
    "            if processed[point_idx]:\n",
    "                continue\n",
    "            self._expand_cluster_order(X, point_idx, processed, seeds, counter)\n",
    "\n",
    "        # extract xi clusters\n",
    "        self._extract_xi_clusters()\n",
    "        return self\n",
    "\n",
    "    def _expand_cluster_order(self, X, point_idx, processed, seeds, counter):\n",
    "        processed[point_idx] = True\n",
    "        self.ordering_.append(point_idx)\n",
    "\n",
    "        if self.core_distances_[point_idx] == np.inf:\n",
    "            return\n",
    "\n",
    "        neighbors = self._get_neighbors(X, point_idx)\n",
    "        self._update_seeds(X, point_idx, neighbors, processed, seeds, counter)\n",
    "\n",
    "        while seeds:\n",
    "            _, _, current_idx = heapq.heappop(seeds)\n",
    "            if processed[current_idx]:\n",
    "                continue\n",
    "\n",
    "            processed[current_idx] = True\n",
    "            self.ordering_.append(current_idx)\n",
    "            self.predecessor_[current_idx] = point_idx\n",
    "\n",
    "            if self.core_distances_[current_idx] != np.inf:\n",
    "                nbrs = self._get_neighbors(X, current_idx)\n",
    "                self._update_seeds(X, current_idx, nbrs, processed, seeds, counter)\n",
    "\n",
    "    def _get_neighbors(self, X, center):\n",
    "        dists = np.linalg.norm(X - X[center], axis=1)\n",
    "        return np.where(dists <= self.max_eps)[0]\n",
    "\n",
    "    # === Eq. 11: Modified Reachability Distance (ORD) ===\n",
    "    def _update_seeds(self, X, center_idx, neighbors, processed, seeds, counter):\n",
    "        for idx in neighbors:\n",
    "            if processed[idx]:\n",
    "                continue\n",
    "\n",
    "            dist = np.linalg.norm(X[center_idx] - X[idx])\n",
    "            # Modified OPTICS reachability\n",
    "            new_reach = max(self.core_distances_[center_idx], dist) / max(self.ri_[center_idx], 1e-9)\n",
    "\n",
    "            if self.reachability_[idx] == np.inf or new_reach < self.reachability_[idx]:\n",
    "                self.reachability_[idx] = new_reach\n",
    "                heapq.heappush(seeds, (new_reach, counter, idx))\n",
    "                counter += 1\n",
    "\n",
    "    def _extract_xi_clusters(self):\n",
    "        # Simple xi-based cluster extraction (kept same as baseline)\n",
    "        n = len(self.reachability_)\n",
    "        ordering = np.array(self.ordering_, dtype=int)\n",
    "        R = np.r_[self.reachability_[ordering], np.inf]\n",
    "        xi_comp = 1.0 - self.xi\n",
    "\n",
    "        labels_ord = np.full(n, -1, dtype=int)\n",
    "        lab = 0\n",
    "        start = 0\n",
    "\n",
    "        for i in range(1, len(R)):\n",
    "            if R[i] / R[i - 1] < xi_comp:  # steep up \n",
    "                if i - start >= self.min_samples:\n",
    "                    labels_ord[start:i] = lab\n",
    "                    lab += 1\n",
    "                start = i\n",
    "\n",
    "        labels = np.full(n, -1, dtype=int)\n",
    "        labels[ordering] = labels_ord\n",
    "        self.labels_ = labels\n",
    "\n",
    "    def fit_predict(self, X, y):\n",
    "        return self.fit(X, y).labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4699212-fd38-4e6b-8f0e-2d17cd065ed4",
   "metadata": {},
   "source": [
    "# Gaussian Augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9d33e6-2d46-462b-9b4d-42d4ddac7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianAugmenter:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = np.random.RandomState(random_state)\n",
    "\n",
    "    def generate_for_clusters(self, X, clusters, y, ri, cluster_ids, n_samples_per_cluster):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples for the given clusters using Multivariate Gaussian noise.\n",
    "        Prioritize boundary minority samples (smallest ri).\n",
    "        \"\"\"\n",
    "        synth_rows = []\n",
    "        synth_labels = []\n",
    "\n",
    "        for cid, n_to_gen in n_samples_per_cluster.items():\n",
    "            if n_to_gen <= 0:\n",
    "                continue\n",
    "\n",
    "            cluster_mask = (clusters == cid)\n",
    "            X_cluster = X[cluster_mask]\n",
    "            y_cluster = y[cluster_mask]\n",
    "            ri_cluster = ri[cluster_mask]\n",
    "\n",
    "            # Focus only on minority in this cluster\n",
    "            minority_mask = (y_cluster == 1)\n",
    "            X_minority = X_cluster[minority_mask]\n",
    "            ri_minority = ri_cluster[minority_mask]\n",
    "\n",
    "            if len(X_minority) < 5:  # stricter cutoff\n",
    "                continue\n",
    "\n",
    "            # Select subset (boundary-like minority samples: smallest ri)\n",
    "            k = max(2, int(0.3 * len(X_minority)))\n",
    "            k = min(k, len(X_minority))\n",
    "            # Sort by ri ascending (smallest first = boundary)\n",
    "            sorted_idx = np.argsort(ri_minority)[:k]\n",
    "            if isinstance(X_minority, pd.DataFrame):\n",
    "                Xb = X_minority.iloc[sorted_idx]\n",
    "            else:\n",
    "                Xb = X_minority[sorted_idx]\n",
    "\n",
    "            if Xb.shape[0] < 2:  # covariance needs at least 2\n",
    "                continue\n",
    "\n",
    "            mu = Xb.mean(axis=0).values if isinstance(Xb, pd.DataFrame) else Xb.mean(axis=0)\n",
    "            cov = np.cov(Xb.values if isinstance(Xb, pd.DataFrame) else Xb, rowvar=False)\n",
    "            cov = np.atleast_2d(cov) + np.eye(cov.shape[0]) * 1e-6  # regularize\n",
    "\n",
    "            for _ in range(n_to_gen):\n",
    "                synth_sample = self.random_state.multivariate_normal(mu, cov)\n",
    "                synth_rows.append(synth_sample)\n",
    "                synth_labels.append(1)\n",
    "\n",
    "        if not synth_rows:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        synth_df = pd.DataFrame(synth_rows, columns=X.columns)\n",
    "        synth_df[\"target\"] = synth_labels\n",
    "        return synth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34f518-28bf-4f88-9ca4-4089a2d58fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ecoli1-5-fold/ecoli1-5-1tra.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 227\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    226\u001b[39m     mm = MainMethod(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[43mmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_5fold_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 211\u001b[39m, in \u001b[36mMainMethod.run_5fold_cv\u001b[39m\u001b[34m(self, fold_prefix)\u001b[39m\n\u001b[32m    209\u001b[39m train_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mtra.dat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m test_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mtst.dat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m before, after = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m                             \u001b[49m\u001b[43moptics_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m                             \u001b[49m\u001b[43moptics_xi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mratio_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmin_minority_in_cluster\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m                             \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m before_list.append(before)\n\u001b[32m    218\u001b[39m after_list.append(after)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mMainMethod.run_fold\u001b[39m\u001b[34m(self, fold_no, train_file, test_file, optics_min_samples, optics_xi, ratio_threshold, min_minority_in_cluster, alpha)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_fold\u001b[39m(\u001b[38;5;28mself\u001b[39m,fold_no,train_file, test_file,\n\u001b[32m     94\u001b[39m              optics_min_samples=\u001b[32m7\u001b[39m, optics_xi=\u001b[32m0.05\u001b[39m,\n\u001b[32m     95\u001b[39m              ratio_threshold=\u001b[32m0.6\u001b[39m, min_minority_in_cluster=\u001b[32m10\u001b[39m, alpha=\u001b[32m4.0\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     X_train, y_train = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     X_test, y_test = \u001b[38;5;28mself\u001b[39m._load_and_prepare_data(test_file)\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Processing Fold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mMainMethod._load_and_prepare_data\u001b[39m\u001b[34m(self, file_path)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_and_prepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[32m     10\u001b[39m     clean_lines = []\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     13\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.strip().lower().startswith(\u001b[33m\"\u001b[39m\u001b[33m@input\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.strip().lower().startswith(\u001b[33m\"\u001b[39m\u001b[33m@output\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'ecoli1-5-fold/ecoli1-5-1tra.dat'"
     ]
    }
   ],
   "source": [
    "class MainMethod:\n",
    "    \"\"\"\n",
    "    Main class to run the augmentation and evaluation pipeline,\n",
    "    reporting only overall, minority, and majority accuracy metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _load_and_prepare_data(self, file_path):\n",
    "        clean_lines = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip().lower().startswith(\"@input\") and not line.strip().lower().startswith(\"@output\"):\n",
    "                    clean_lines.append(line)\n",
    "        temp_arff = file_path.replace('.dat', '.arff')\n",
    "        with open(temp_arff, \"w\") as f:\n",
    "            f.writelines(clean_lines)\n",
    "        data, meta = arff.loadarff(temp_arff)\n",
    "        df = pd.DataFrame(data)\n",
    "        # Dynamically find class column (usually last attribute in KEEL datasets)\n",
    "        class_col = meta.names()[-1] if meta is not None and len(meta.names()) > 0 else None\n",
    "        if class_col is None or class_col not in df.columns:\n",
    "            print(f\"Error: No class column found in {file_path}. Available columns: {list(df.columns)}\")\n",
    "            raise KeyError(f\"No class column found in {file_path}. Available columns: {list(df.columns)}\")\n",
    "        # Decode bytes if necessary\n",
    "        df[class_col] = df[class_col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "        le = LabelEncoder()\n",
    "        df['Class_encoded'] = le.fit_transform(df[class_col])\n",
    "        X = df.drop(columns=[class_col, 'Class_encoded'])\n",
    "        y = df['Class_encoded'].values.ravel()\n",
    "        os.remove(temp_arff)\n",
    "        return X, y\n",
    "\n",
    "    def _compute_metrics(self, y_true, y_pred, y_proba=None):\n",
    "        '''\n",
    "        Only computes and returns:\n",
    "            overall_accuracy, minority_accuracy, majority_accuracy\n",
    "        '''\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        # Assuming binary classification: minority class = 1, majority = 0\n",
    "        majority_true = (y_true == 0).sum()\n",
    "        minority_true = (y_true == 1).sum()\n",
    "        majority_correct = tn\n",
    "        minority_correct = tp\n",
    "        majority_acc = majority_correct / majority_true if majority_true > 0 else 0\n",
    "        minority_acc = minority_correct / minority_true if minority_true > 0 else 0\n",
    "        return {\n",
    "            'overall_accuracy': acc,\n",
    "            'majority_accuracy': majority_acc,\n",
    "            'minority_accuracy': minority_acc\n",
    "        }\n",
    "\n",
    "    def _print_metrics(self, metrics, title):\n",
    "        print(f\"\\n{'=' * 30} {title} {'=' * 30}\")\n",
    "        print(f\"{'Metric':<20} {'Value':<10}\")\n",
    "        print(\"-\" * 35)\n",
    "        for metric, value in metrics.items():\n",
    "            if value is not None:\n",
    "                print(f\"{metric:<20} {value:.3f}\")\n",
    "\n",
    "    def _print_cluster_stats(self, X_train_df, selected_clusters, n_samples_per_cluster):\n",
    "        \"\"\"\n",
    "        Print statistics for all clusters, including noise cluster (-1), regardless of min_minority_in_cluster eligibility.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'=' * 30} Cluster Statistics (Noise as Cluster -1) {'=' * 30}\")\n",
    "        # Calculate % Noise\n",
    "        noise_mask = (X_train_df[\"cluster\"] == -1)\n",
    "        noise_size = len(X_train_df[noise_mask])\n",
    "        total_size = len(X_train_df)\n",
    "        percent_noise = (noise_size / total_size * 100) if total_size > 0 else 0.0\n",
    "        print(f\"% Noise: {percent_noise:.2f}%\")\n",
    "        print(f\"{'Cluster ID':<12} {'Minority':<10} {'Majority':<10} {'Ratio':<10} {'Samples to Gen':<15} {'Size':<10}\")\n",
    "        print(\"-\" * 67)\n",
    "        # Get all unique cluster IDs, including -1\n",
    "        all_clusters = np.unique(X_train_df[\"cluster\"])\n",
    "        for cid in sorted(all_clusters, key=lambda x: (x != -1, x)):\n",
    "            mask = (X_train_df[\"cluster\"] == cid)\n",
    "            n_minority = sum(X_train_df[mask][\"target\"] == 1)\n",
    "            n_majority = sum(X_train_df[mask][\"target\"] == 0)\n",
    "            ratio = n_minority / max(n_majority, 1)\n",
    "            cluster_size = n_minority + n_majority\n",
    "            n_to_gen = n_samples_per_cluster.get(cid, 0)\n",
    "            print(f\"{cid:<12} {n_minority:<10} {n_majority:<10} {ratio:.3f}    {n_to_gen:<15} {cluster_size:<10}\")\n",
    "        print(\"-\" * 67)\n",
    "        total_minority = sum(X_train_df[\"target\"] == 1)\n",
    "        total_majority = sum(X_train_df[\"target\"] == 0)\n",
    "        total_ratio = total_minority / max(total_majority, 1)\n",
    "        total_size = total_minority + total_majority\n",
    "        total_samples_to_gen = sum(n_samples_per_cluster.values())\n",
    "        print(f\"{'Total':<12} {total_minority:<10} {total_majority:<10} {total_ratio:.3f}    {total_samples_to_gen:<15} {total_size:<10}\")\n",
    "\n",
    "    def run_fold(self,fold_no,train_file, test_file,\n",
    "                 optics_min_samples=7, optics_xi=0.05,\n",
    "                 ratio_threshold=0.6, min_minority_in_cluster=10, alpha=4.0):\n",
    "        X_train, y_train = self._load_and_prepare_data(train_file)\n",
    "        X_test, y_test = self._load_and_prepare_data(test_file)\n",
    "        print(f\"\\n{'=' * 30} Processing Fold-{fold_no}: Train size={len(X_train)}, Test size={len(X_test)} {'=' * 30}\")\n",
    "\n",
    "        # Before augmentation\n",
    "        clf = RandomForestClassifier(random_state=self.random_state)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        before_metrics = self._compute_metrics(y_test, y_pred, y_proba)\n",
    "        self._print_metrics(before_metrics, \"Before Augmentation\")\n",
    "\n",
    "        # Clustering\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        optics = ModifiedOPTICS(min_samples=optics_min_samples, xi=optics_xi, max_eps=np.inf)\n",
    "        clusters = optics.fit_predict(X_train_scaled, y_train)\n",
    "        X_train_df = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "        X_train_df[\"target\"] = y_train\n",
    "        X_train_df[\"cluster\"] = clusters\n",
    "\n",
    "        selected_clusters = np.unique(clusters[clusters >= 0])\n",
    "        noise_mask = (X_train_df[\"cluster\"] == -1)\n",
    "        n_min_noise = sum(X_train_df[noise_mask][\"target\"] == 1)\n",
    "        n_maj_noise = sum(X_train_df[noise_mask][\"target\"] == 0)\n",
    "        if n_min_noise >= min_minority_in_cluster:\n",
    "            selected_clusters = np.append(selected_clusters, -1)\n",
    "\n",
    "        # Determine samples to generate\n",
    "        augmenter = GaussianAugmenter(random_state=self.random_state)\n",
    "        n_samples_per_cluster = {}\n",
    "        for cid in selected_clusters:\n",
    "            mask = (X_train_df[\"cluster\"] == cid)\n",
    "            n_minority = sum(X_train_df[mask][\"target\"] == 1)\n",
    "            n_majority = sum(X_train_df[mask][\"target\"] == 0)\n",
    "            ratio = n_minority / max(n_majority, 1)\n",
    "            # print(f\"Cluster {cid}: Minority={n_minority}, Majority={n_majority}, Ratio={ratio:.3f}\")\n",
    "            if n_minority < min_minority_in_cluster:\n",
    "                n_samples_per_cluster[cid] = 0\n",
    "                continue\n",
    "            if ratio < ratio_threshold:\n",
    "                n_samples_per_cluster[cid] = 0\n",
    "                continue\n",
    "            n_samples_per_cluster[cid] = int(alpha * n_minority)\n",
    "\n",
    "        # Print cluster stats (optional, can be commented)\n",
    "        self._print_cluster_stats(X_train_df, selected_clusters, n_samples_per_cluster)\n",
    "\n",
    "        # Generate synthetic samples\n",
    "        synth_df = augmenter.generate_for_clusters(\n",
    "            X=X_train_df.drop(columns=[\"target\", \"cluster\"]),\n",
    "            clusters=X_train_df[\"cluster\"].values,\n",
    "            y=X_train_df[\"target\"].values,\n",
    "            ri=optics.ri_,\n",
    "            cluster_ids=selected_clusters,\n",
    "            n_samples_per_cluster=n_samples_per_cluster,\n",
    "        )\n",
    "\n",
    "        if not synth_df.empty:\n",
    "            maj = (y_train == 0).sum()\n",
    "            min_now = (y_train == 1).sum()\n",
    "            max_add = max(0, int(1.0 * maj - min_now))\n",
    "            if len(synth_df) > max_add:\n",
    "                synth_df = synth_df.iloc[:max_add]\n",
    "            X_train_aug = pd.concat([\n",
    "                X_train_df.drop(columns=[\"target\", \"cluster\"]),\n",
    "                synth_df.drop(columns=[\"target\"])\n",
    "            ])\n",
    "            y_train_aug = np.concatenate([\n",
    "                X_train_df[\"target\"].values,\n",
    "                synth_df[\"target\"].values\n",
    "            ])\n",
    "            print(f\"Generated synthetic samples: {len(synth_df)}\")\n",
    "        else:\n",
    "            X_train_aug = X_train_df.drop(columns=[\"target\", \"cluster\"])\n",
    "            y_train_aug = X_train_df[\"target\"].values\n",
    "            print(\"No synthetic samples generated.\")\n",
    "\n",
    "        # Class distribution\n",
    "        before_counts = Counter(y_train)\n",
    "        after_counts = Counter(y_train_aug)\n",
    "        minority_class = 1\n",
    "        majority_class = 0\n",
    "        before_ratio = before_counts[majority_class]/before_counts[minority_class] if before_counts[majority_class] > 0 else 0\n",
    "        after_ratio = after_counts[majority_class]/after_counts[minority_class]  if after_counts[majority_class] > 0 else 0\n",
    "        print(f\"\\nClass Distribution:\")\n",
    "        print(f\"Before - Minority: {before_counts[minority_class]}, Majority: {before_counts[majority_class]}, Ratio: {before_ratio:.3f}\")\n",
    "        print(f\"After  - Minority: {after_counts[minority_class]}, Majority: {after_counts[majority_class]}, Ratio: {after_ratio:.3f}\")\n",
    "\n",
    "        # After augmentation\n",
    "        clf = RandomForestClassifier(random_state=self.random_state)\n",
    "        clf.fit(X_train_aug, y_train_aug)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        after_metrics = self._compute_metrics(y_test, y_pred, y_proba)\n",
    "        self._print_metrics(after_metrics, \"After Augmentation\")\n",
    "\n",
    "        return before_metrics, after_metrics\n",
    "\n",
    "    def _print_average_metrics(self, avg_before, avg_after, avg_gain):\n",
    "        print(f\"\\n{'=' * 30} Average Results Across Folds {'=' * 30}\")\n",
    "        print(f\"{'Metric':<20} {'Before':<10} {'After':<10} {'Gain':<10} {'Relative % Gain':<15}\")\n",
    "        print(\"-\" * 65)\n",
    "        for metric in avg_before.keys():\n",
    "            if avg_before[metric] is not None:\n",
    "                relative_gain = (avg_gain[metric] / avg_before[metric] * 100) if avg_before[metric] != 0 else 0.0\n",
    "                print(f\"{metric:<20} {avg_before[metric]:.3f}     {avg_after[metric]:.3f}     {avg_gain[metric]:.3f}     {relative_gain:.2f}%\")\n",
    "\n",
    "    def run_5fold_cv(self, fold_prefix='ecoli1-5-fold/ecoli1-5-'):\n",
    "        folds = [1, 2, 3, 4, 5]\n",
    "        before_list = []\n",
    "        after_list = []\n",
    "        for fold in folds:\n",
    "            train_file = f\"{fold_prefix}{fold}tra.dat\"\n",
    "            test_file = f\"{fold_prefix}{fold}tst.dat\"\n",
    "            before, after = self.run_fold(fold,train_file, test_file,\n",
    "                                         optics_min_samples=8,\n",
    "                                         optics_xi=0.05,\n",
    "                                         ratio_threshold=0.6,\n",
    "                                         min_minority_in_cluster=6,\n",
    "                                         alpha=1.0)\n",
    "            before_list.append(before)\n",
    "            after_list.append(after)\n",
    "        # Use only the three focused metrics for output\n",
    "        avg_before = {k: np.mean([d[k] for d in before_list if d[k] is not None]) for k in before_list[0]}\n",
    "        avg_after = {k: np.mean([d[k] for d in after_list if d[k] is not None]) for k in after_list[0]}\n",
    "        avg_gain = {k: avg_after[k] - avg_before[k] for k in avg_before}\n",
    "        self._print_average_metrics(avg_before, avg_after, avg_gain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mm = MainMethod(random_state=42)\n",
    "    mm.run_5fold_cv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cdd14-0942-4d9e-b311-dca55be15943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
